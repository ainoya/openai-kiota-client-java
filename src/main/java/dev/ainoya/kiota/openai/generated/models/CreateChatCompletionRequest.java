package dev.ainoya.kiota.openai.generated.models;

import com.microsoft.kiota.serialization.AdditionalDataHolder;
import com.microsoft.kiota.serialization.ComposedTypeWrapper;
import com.microsoft.kiota.serialization.Parsable;
import com.microsoft.kiota.serialization.ParseNode;
import com.microsoft.kiota.serialization.SerializationWriter;
import java.util.HashMap;
import java.util.Map;
import java.util.Objects;
@jakarta.annotation.Generated("com.microsoft.kiota")
public class CreateChatCompletionRequest implements AdditionalDataHolder, Parsable {
    /**
     * Stores additional data not described in the OpenAPI description found when deserializing. Can be used for serialization as well.
     */
    private Map<String, Object> additionalData;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
     */
    private Double frequencyPenalty;
    /**
     * Deprecated in favor of `tool_choice`.Controls which (if any) function is called by the model.`none` means the model will not call a function and instead generates a message.`auto` means the model can pick between generating a message or calling a function.Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.`none` is the default when no functions are present. `auto` is the default if functions are present.
     * @deprecated
     * 
     */
    @Deprecated
    private CreateChatCompletionRequestFunctionCall functionCall;
    /**
     * Deprecated in favor of `tools`.A list of functions the model may generate JSON inputs for.
     * @deprecated
     * 
     */
    @Deprecated
    private java.util.List<ChatCompletionFunctions> functions;
    /**
     * Modify the likelihood of specified tokens appearing in the completion.Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
     */
    private CreateChatCompletionRequestLogitBias logitBias;
    /**
     * Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. This option is currently not available on the `gpt-4-vision-preview` model.
     */
    private Boolean logprobs;
    /**
     * The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
     */
    private Integer maxTokens;
    /**
     * A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
     */
    private java.util.List<ChatCompletionRequestMessage> messages;
    /**
     * ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.
     */
    private CreateChatCompletionRequestModel model;
    /**
     * How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
     */
    private Integer n;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
     */
    private Double presencePenalty;
    /**
     * An object specifying the format that the model must output. Compatible with [GPT-4 Turbo](/docs/models/gpt-4-and-gpt-4-turbo) and `gpt-3.5-turbo-1106`.Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.**Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
     */
    private CreateChatCompletionRequestResponseFormat responseFormat;
    /**
     * This feature is in Beta.If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
     */
    private Integer seed;
    /**
     * Up to 4 sequences where the API will stop generating further tokens.
     */
    private CreateChatCompletionRequestStop stop;
    /**
     * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
     */
    private Boolean stream;
    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.We generally recommend altering this or `top_p` but not both.
     */
    private Double temperature;
    /**
     * Controls which (if any) function is called by the model.`none` means the model will not call a function and instead generates a message.`auto` means the model can pick between generating a message or calling a function.Specifying a particular function via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that function.`none` is the default when no functions are present. `auto` is the default if functions are present.
     */
    private ChatCompletionNamedToolChoice toolChoice;
    /**
     * A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.
     */
    private java.util.List<ChatCompletionTool> tools;
    /**
     * An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
     */
    private Integer topLogprobs;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.We generally recommend altering this or `temperature` but not both.
     */
    private Double topP;
    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
     */
    private String user;
    /**
     * Instantiates a new CreateChatCompletionRequest and sets the default values.
     */
    public CreateChatCompletionRequest() {
        this.setAdditionalData(new HashMap<>());
    }
    /**
     * Creates a new instance of the appropriate class based on discriminator value
     * @param parseNode The parse node to use to read the discriminator value and create the object
     * @return a CreateChatCompletionRequest
     */
    @jakarta.annotation.Nonnull
    public static CreateChatCompletionRequest createFromDiscriminatorValue(@jakarta.annotation.Nonnull final ParseNode parseNode) {
        Objects.requireNonNull(parseNode);
        return new CreateChatCompletionRequest();
    }
    /**
     * Gets the AdditionalData property value. Stores additional data not described in the OpenAPI description found when deserializing. Can be used for serialization as well.
     * @return a Map<String, Object>
     */
    @jakarta.annotation.Nonnull
    public Map<String, Object> getAdditionalData() {
        return this.additionalData;
    }
    /**
     * The deserialization information for the current model
     * @return a Map<String, java.util.function.Consumer<ParseNode>>
     */
    @jakarta.annotation.Nonnull
    public Map<String, java.util.function.Consumer<ParseNode>> getFieldDeserializers() {
        final HashMap<String, java.util.function.Consumer<ParseNode>> deserializerMap = new HashMap<String, java.util.function.Consumer<ParseNode>>(20);
        deserializerMap.put("frequency_penalty", (n) -> { this.setFrequencyPenalty(n.getDoubleValue()); });
        deserializerMap.put("function_call", (n) -> { this.setFunctionCall(n.getObjectValue(CreateChatCompletionRequestFunctionCall::createFromDiscriminatorValue)); });
        deserializerMap.put("functions", (n) -> { this.setFunctions(n.getCollectionOfObjectValues(ChatCompletionFunctions::createFromDiscriminatorValue)); });
        deserializerMap.put("logit_bias", (n) -> { this.setLogitBias(n.getObjectValue(CreateChatCompletionRequestLogitBias::createFromDiscriminatorValue)); });
        deserializerMap.put("logprobs", (n) -> { this.setLogprobs(n.getBooleanValue()); });
        deserializerMap.put("max_tokens", (n) -> { this.setMaxTokens(n.getIntegerValue()); });
        deserializerMap.put("messages", (n) -> { this.setMessages(n.getCollectionOfObjectValues(ChatCompletionRequestMessage::createFromDiscriminatorValue)); });
        deserializerMap.put("model", (n) -> { this.setModel(n.getObjectValue(CreateChatCompletionRequestModel::createFromDiscriminatorValue)); });
        deserializerMap.put("n", (n) -> { this.setN(n.getIntegerValue()); });
        deserializerMap.put("presence_penalty", (n) -> { this.setPresencePenalty(n.getDoubleValue()); });
        deserializerMap.put("response_format", (n) -> { this.setResponseFormat(n.getObjectValue(CreateChatCompletionRequestResponseFormat::createFromDiscriminatorValue)); });
        deserializerMap.put("seed", (n) -> { this.setSeed(n.getIntegerValue()); });
        deserializerMap.put("stop", (n) -> { this.setStop(n.getObjectValue(CreateChatCompletionRequestStop::createFromDiscriminatorValue)); });
        deserializerMap.put("stream", (n) -> { this.setStream(n.getBooleanValue()); });
        deserializerMap.put("temperature", (n) -> { this.setTemperature(n.getDoubleValue()); });
        deserializerMap.put("tool_choice", (n) -> { this.setToolChoice(n.getObjectValue(ChatCompletionNamedToolChoice::createFromDiscriminatorValue)); });
        deserializerMap.put("tools", (n) -> { this.setTools(n.getCollectionOfObjectValues(ChatCompletionTool::createFromDiscriminatorValue)); });
        deserializerMap.put("top_logprobs", (n) -> { this.setTopLogprobs(n.getIntegerValue()); });
        deserializerMap.put("top_p", (n) -> { this.setTopP(n.getDoubleValue()); });
        deserializerMap.put("user", (n) -> { this.setUser(n.getStringValue()); });
        return deserializerMap;
    }
    /**
     * Gets the frequency_penalty property value. Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
     * @return a Double
     */
    @jakarta.annotation.Nullable
    public Double getFrequencyPenalty() {
        return this.frequencyPenalty;
    }
    /**
     * Gets the function_call property value. Deprecated in favor of `tool_choice`.Controls which (if any) function is called by the model.`none` means the model will not call a function and instead generates a message.`auto` means the model can pick between generating a message or calling a function.Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.`none` is the default when no functions are present. `auto` is the default if functions are present.
     * @return a CreateChatCompletionRequestFunctionCall
     * @deprecated
     * 
     */
    @jakarta.annotation.Nullable
    @Deprecated
    public CreateChatCompletionRequestFunctionCall getFunctionCall() {
        return this.functionCall;
    }
    /**
     * Gets the functions property value. Deprecated in favor of `tools`.A list of functions the model may generate JSON inputs for.
     * @return a java.util.List<ChatCompletionFunctions>
     * @deprecated
     * 
     */
    @jakarta.annotation.Nullable
    @Deprecated
    public java.util.List<ChatCompletionFunctions> getFunctions() {
        return this.functions;
    }
    /**
     * Gets the logit_bias property value. Modify the likelihood of specified tokens appearing in the completion.Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
     * @return a CreateChatCompletionRequestLogitBias
     */
    @jakarta.annotation.Nullable
    public CreateChatCompletionRequestLogitBias getLogitBias() {
        return this.logitBias;
    }
    /**
     * Gets the logprobs property value. Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. This option is currently not available on the `gpt-4-vision-preview` model.
     * @return a Boolean
     */
    @jakarta.annotation.Nullable
    public Boolean getLogprobs() {
        return this.logprobs;
    }
    /**
     * Gets the max_tokens property value. The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
     * @return a Integer
     */
    @jakarta.annotation.Nullable
    public Integer getMaxTokens() {
        return this.maxTokens;
    }
    /**
     * Gets the messages property value. A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
     * @return a java.util.List<ChatCompletionRequestMessage>
     */
    @jakarta.annotation.Nullable
    public java.util.List<ChatCompletionRequestMessage> getMessages() {
        return this.messages;
    }
    /**
     * Gets the model property value. ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.
     * @return a CreateChatCompletionRequestModel
     */
    @jakarta.annotation.Nullable
    public CreateChatCompletionRequestModel getModel() {
        return this.model;
    }
    /**
     * Gets the n property value. How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
     * @return a Integer
     */
    @jakarta.annotation.Nullable
    public Integer getN() {
        return this.n;
    }
    /**
     * Gets the presence_penalty property value. Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
     * @return a Double
     */
    @jakarta.annotation.Nullable
    public Double getPresencePenalty() {
        return this.presencePenalty;
    }
    /**
     * Gets the response_format property value. An object specifying the format that the model must output. Compatible with [GPT-4 Turbo](/docs/models/gpt-4-and-gpt-4-turbo) and `gpt-3.5-turbo-1106`.Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.**Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
     * @return a CreateChatCompletionRequestResponseFormat
     */
    @jakarta.annotation.Nullable
    public CreateChatCompletionRequestResponseFormat getResponseFormat() {
        return this.responseFormat;
    }
    /**
     * Gets the seed property value. This feature is in Beta.If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
     * @return a Integer
     */
    @jakarta.annotation.Nullable
    public Integer getSeed() {
        return this.seed;
    }
    /**
     * Gets the stop property value. Up to 4 sequences where the API will stop generating further tokens.
     * @return a CreateChatCompletionRequestStop
     */
    @jakarta.annotation.Nullable
    public CreateChatCompletionRequestStop getStop() {
        return this.stop;
    }
    /**
     * Gets the stream property value. If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
     * @return a Boolean
     */
    @jakarta.annotation.Nullable
    public Boolean getStream() {
        return this.stream;
    }
    /**
     * Gets the temperature property value. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.We generally recommend altering this or `top_p` but not both.
     * @return a Double
     */
    @jakarta.annotation.Nullable
    public Double getTemperature() {
        return this.temperature;
    }
    /**
     * Gets the tool_choice property value. Controls which (if any) function is called by the model.`none` means the model will not call a function and instead generates a message.`auto` means the model can pick between generating a message or calling a function.Specifying a particular function via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that function.`none` is the default when no functions are present. `auto` is the default if functions are present.
     * @return a ChatCompletionNamedToolChoice
     */
    @jakarta.annotation.Nullable
    public ChatCompletionNamedToolChoice getToolChoice() {
        return this.toolChoice;
    }
    /**
     * Gets the tools property value. A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.
     * @return a java.util.List<ChatCompletionTool>
     */
    @jakarta.annotation.Nullable
    public java.util.List<ChatCompletionTool> getTools() {
        return this.tools;
    }
    /**
     * Gets the top_logprobs property value. An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
     * @return a Integer
     */
    @jakarta.annotation.Nullable
    public Integer getTopLogprobs() {
        return this.topLogprobs;
    }
    /**
     * Gets the top_p property value. An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.We generally recommend altering this or `temperature` but not both.
     * @return a Double
     */
    @jakarta.annotation.Nullable
    public Double getTopP() {
        return this.topP;
    }
    /**
     * Gets the user property value. A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
     * @return a String
     */
    @jakarta.annotation.Nullable
    public String getUser() {
        return this.user;
    }
    /**
     * Serializes information the current object
     * @param writer Serialization writer to use to serialize this model
     */
    public void serialize(@jakarta.annotation.Nonnull final SerializationWriter writer) {
        Objects.requireNonNull(writer);
        writer.writeDoubleValue("frequency_penalty", this.getFrequencyPenalty());
        writer.writeObjectValue("function_call", this.getFunctionCall());
        writer.writeCollectionOfObjectValues("functions", this.getFunctions());
        writer.writeObjectValue("logit_bias", this.getLogitBias());
        writer.writeBooleanValue("logprobs", this.getLogprobs());
        writer.writeIntegerValue("max_tokens", this.getMaxTokens());
        writer.writeCollectionOfObjectValues("messages", this.getMessages());
        writer.writeObjectValue("model", this.getModel());
        writer.writeIntegerValue("n", this.getN());
        writer.writeDoubleValue("presence_penalty", this.getPresencePenalty());
        writer.writeObjectValue("response_format", this.getResponseFormat());
        writer.writeIntegerValue("seed", this.getSeed());
        writer.writeObjectValue("stop", this.getStop());
        writer.writeBooleanValue("stream", this.getStream());
        writer.writeDoubleValue("temperature", this.getTemperature());
        writer.writeObjectValue("tool_choice", this.getToolChoice());
        writer.writeCollectionOfObjectValues("tools", this.getTools());
        writer.writeIntegerValue("top_logprobs", this.getTopLogprobs());
        writer.writeDoubleValue("top_p", this.getTopP());
        writer.writeStringValue("user", this.getUser());
        writer.writeAdditionalData(this.getAdditionalData());
    }
    /**
     * Sets the AdditionalData property value. Stores additional data not described in the OpenAPI description found when deserializing. Can be used for serialization as well.
     * @param value Value to set for the AdditionalData property.
     */
    public void setAdditionalData(@jakarta.annotation.Nullable final Map<String, Object> value) {
        this.additionalData = value;
    }
    /**
     * Sets the frequency_penalty property value. Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
     * @param value Value to set for the frequency_penalty property.
     */
    public void setFrequencyPenalty(@jakarta.annotation.Nullable final Double value) {
        this.frequencyPenalty = value;
    }
    /**
     * Sets the function_call property value. Deprecated in favor of `tool_choice`.Controls which (if any) function is called by the model.`none` means the model will not call a function and instead generates a message.`auto` means the model can pick between generating a message or calling a function.Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.`none` is the default when no functions are present. `auto` is the default if functions are present.
     * @param value Value to set for the function_call property.
     * @deprecated
     * 
     */
    @Deprecated
    public void setFunctionCall(@jakarta.annotation.Nullable final CreateChatCompletionRequestFunctionCall value) {
        this.functionCall = value;
    }
    /**
     * Sets the functions property value. Deprecated in favor of `tools`.A list of functions the model may generate JSON inputs for.
     * @param value Value to set for the functions property.
     * @deprecated
     * 
     */
    @Deprecated
    public void setFunctions(@jakarta.annotation.Nullable final java.util.List<ChatCompletionFunctions> value) {
        this.functions = value;
    }
    /**
     * Sets the logit_bias property value. Modify the likelihood of specified tokens appearing in the completion.Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
     * @param value Value to set for the logit_bias property.
     */
    public void setLogitBias(@jakarta.annotation.Nullable final CreateChatCompletionRequestLogitBias value) {
        this.logitBias = value;
    }
    /**
     * Sets the logprobs property value. Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. This option is currently not available on the `gpt-4-vision-preview` model.
     * @param value Value to set for the logprobs property.
     */
    public void setLogprobs(@jakarta.annotation.Nullable final Boolean value) {
        this.logprobs = value;
    }
    /**
     * Sets the max_tokens property value. The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
     * @param value Value to set for the max_tokens property.
     */
    public void setMaxTokens(@jakarta.annotation.Nullable final Integer value) {
        this.maxTokens = value;
    }
    /**
     * Sets the messages property value. A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
     * @param value Value to set for the messages property.
     */
    public void setMessages(@jakarta.annotation.Nullable final java.util.List<ChatCompletionRequestMessage> value) {
        this.messages = value;
    }
    /**
     * Sets the model property value. ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.
     * @param value Value to set for the model property.
     */
    public void setModel(@jakarta.annotation.Nullable final CreateChatCompletionRequestModel value) {
        this.model = value;
    }
    /**
     * Sets the n property value. How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
     * @param value Value to set for the n property.
     */
    public void setN(@jakarta.annotation.Nullable final Integer value) {
        this.n = value;
    }
    /**
     * Sets the presence_penalty property value. Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
     * @param value Value to set for the presence_penalty property.
     */
    public void setPresencePenalty(@jakarta.annotation.Nullable final Double value) {
        this.presencePenalty = value;
    }
    /**
     * Sets the response_format property value. An object specifying the format that the model must output. Compatible with [GPT-4 Turbo](/docs/models/gpt-4-and-gpt-4-turbo) and `gpt-3.5-turbo-1106`.Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.**Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
     * @param value Value to set for the response_format property.
     */
    public void setResponseFormat(@jakarta.annotation.Nullable final CreateChatCompletionRequestResponseFormat value) {
        this.responseFormat = value;
    }
    /**
     * Sets the seed property value. This feature is in Beta.If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
     * @param value Value to set for the seed property.
     */
    public void setSeed(@jakarta.annotation.Nullable final Integer value) {
        this.seed = value;
    }
    /**
     * Sets the stop property value. Up to 4 sequences where the API will stop generating further tokens.
     * @param value Value to set for the stop property.
     */
    public void setStop(@jakarta.annotation.Nullable final CreateChatCompletionRequestStop value) {
        this.stop = value;
    }
    /**
     * Sets the stream property value. If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
     * @param value Value to set for the stream property.
     */
    public void setStream(@jakarta.annotation.Nullable final Boolean value) {
        this.stream = value;
    }
    /**
     * Sets the temperature property value. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.We generally recommend altering this or `top_p` but not both.
     * @param value Value to set for the temperature property.
     */
    public void setTemperature(@jakarta.annotation.Nullable final Double value) {
        this.temperature = value;
    }
    /**
     * Sets the tool_choice property value. Controls which (if any) function is called by the model.`none` means the model will not call a function and instead generates a message.`auto` means the model can pick between generating a message or calling a function.Specifying a particular function via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that function.`none` is the default when no functions are present. `auto` is the default if functions are present.
     * @param value Value to set for the tool_choice property.
     */
    public void setToolChoice(@jakarta.annotation.Nullable final ChatCompletionNamedToolChoice value) {
        this.toolChoice = value;
    }
    /**
     * Sets the tools property value. A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.
     * @param value Value to set for the tools property.
     */
    public void setTools(@jakarta.annotation.Nullable final java.util.List<ChatCompletionTool> value) {
        this.tools = value;
    }
    /**
     * Sets the top_logprobs property value. An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
     * @param value Value to set for the top_logprobs property.
     */
    public void setTopLogprobs(@jakarta.annotation.Nullable final Integer value) {
        this.topLogprobs = value;
    }
    /**
     * Sets the top_p property value. An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.We generally recommend altering this or `temperature` but not both.
     * @param value Value to set for the top_p property.
     */
    public void setTopP(@jakarta.annotation.Nullable final Double value) {
        this.topP = value;
    }
    /**
     * Sets the user property value. A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
     * @param value Value to set for the user property.
     */
    public void setUser(@jakarta.annotation.Nullable final String value) {
        this.user = value;
    }
    /**
     * Composed type wrapper for classes ChatCompletionFunctionCallOption, string
     */
    @jakarta.annotation.Generated("com.microsoft.kiota")
    public static class CreateChatCompletionRequestFunctionCall implements ComposedTypeWrapper, Parsable {
        /**
         * Composed type representation for type ChatCompletionFunctionCallOption
         */
        private ChatCompletionFunctionCallOption chatCompletionFunctionCallOption;
        /**
         * Composed type representation for type string
         */
        private String string;
        /**
         * Creates a new instance of the appropriate class based on discriminator value
         * @param parseNode The parse node to use to read the discriminator value and create the object
         * @return a CreateChatCompletionRequestFunctionCall
         */
        @jakarta.annotation.Nonnull
        public static CreateChatCompletionRequestFunctionCall createFromDiscriminatorValue(@jakarta.annotation.Nonnull final ParseNode parseNode) {
            Objects.requireNonNull(parseNode);
            final CreateChatCompletionRequestFunctionCall result = new CreateChatCompletionRequestFunctionCall();
            final ParseNode mappingValueNode = parseNode.getChildNode("");
            if (mappingValueNode != null) {
                final String mappingValue = mappingValueNode.getStringValue();
            }
            if (parseNode.getStringValue() != null) {
                result.setString(parseNode.getStringValue());
            }
            return result;
        }
        /**
         * Gets the ChatCompletionFunctionCallOption property value. Composed type representation for type ChatCompletionFunctionCallOption
         * @return a ChatCompletionFunctionCallOption
         */
        @jakarta.annotation.Nullable
        public ChatCompletionFunctionCallOption getChatCompletionFunctionCallOption() {
            return this.chatCompletionFunctionCallOption;
        }
        /**
         * The deserialization information for the current model
         * @return a Map<String, java.util.function.Consumer<ParseNode>>
         */
        @jakarta.annotation.Nonnull
        public Map<String, java.util.function.Consumer<ParseNode>> getFieldDeserializers() {
            if (this.getChatCompletionFunctionCallOption() != null) {
                return this.getChatCompletionFunctionCallOption().getFieldDeserializers();
            }
            return new HashMap<String, java.util.function.Consumer<ParseNode>>();
        }
        /**
         * Gets the string property value. Composed type representation for type string
         * @return a String
         */
        @jakarta.annotation.Nullable
        public String getString() {
            return this.string;
        }
        /**
         * Serializes information the current object
         * @param writer Serialization writer to use to serialize this model
         */
        public void serialize(@jakarta.annotation.Nonnull final SerializationWriter writer) {
            Objects.requireNonNull(writer);
            if (this.getChatCompletionFunctionCallOption() != null) {
                writer.writeObjectValue(null, this.getChatCompletionFunctionCallOption());
            } else if (this.getString() != null) {
                writer.writeStringValue(null, this.getString());
            }
        }
        /**
         * Sets the ChatCompletionFunctionCallOption property value. Composed type representation for type ChatCompletionFunctionCallOption
         * @param value Value to set for the ChatCompletionFunctionCallOption property.
         */
        public void setChatCompletionFunctionCallOption(@jakarta.annotation.Nullable final ChatCompletionFunctionCallOption value) {
            this.chatCompletionFunctionCallOption = value;
        }
        /**
         * Sets the string property value. Composed type representation for type string
         * @param value Value to set for the string property.
         */
        public void setString(@jakarta.annotation.Nullable final String value) {
            this.string = value;
        }
    }
    /**
     * Composed type wrapper for classes string
     */
    @jakarta.annotation.Generated("com.microsoft.kiota")
    public static class CreateChatCompletionRequestModel implements ComposedTypeWrapper, Parsable {
        /**
         * Composed type representation for type string
         */
        private String string;
        /**
         * Creates a new instance of the appropriate class based on discriminator value
         * @param parseNode The parse node to use to read the discriminator value and create the object
         * @return a CreateChatCompletionRequestModel
         */
        @jakarta.annotation.Nonnull
        public static CreateChatCompletionRequestModel createFromDiscriminatorValue(@jakarta.annotation.Nonnull final ParseNode parseNode) {
            Objects.requireNonNull(parseNode);
            final CreateChatCompletionRequestModel result = new CreateChatCompletionRequestModel();
            if (parseNode.getStringValue() != null) {
                result.setString(parseNode.getStringValue());
            }
            return result;
        }
        /**
         * The deserialization information for the current model
         * @return a Map<String, java.util.function.Consumer<ParseNode>>
         */
        @jakarta.annotation.Nonnull
        public Map<String, java.util.function.Consumer<ParseNode>> getFieldDeserializers() {
            return new HashMap<String, java.util.function.Consumer<ParseNode>>();
        }
        /**
         * Gets the string property value. Composed type representation for type string
         * @return a String
         */
        @jakarta.annotation.Nullable
        public String getString() {
            return this.string;
        }
        /**
         * Serializes information the current object
         * @param writer Serialization writer to use to serialize this model
         */
        public void serialize(@jakarta.annotation.Nonnull final SerializationWriter writer) {
            Objects.requireNonNull(writer);
            if (this.getString() != null) {
                writer.writeStringValue(null, this.getString());
            }
        }
        /**
         * Sets the string property value. Composed type representation for type string
         * @param value Value to set for the string property.
         */
        public void setString(@jakarta.annotation.Nullable final String value) {
            this.string = value;
        }
    }
    /**
     * Composed type wrapper for classes string
     */
    @jakarta.annotation.Generated("com.microsoft.kiota")
    public static class CreateChatCompletionRequestStop implements ComposedTypeWrapper, Parsable {
        /**
         * Composed type representation for type string
         */
        private String string;
        /**
         * Creates a new instance of the appropriate class based on discriminator value
         * @param parseNode The parse node to use to read the discriminator value and create the object
         * @return a CreateChatCompletionRequestStop
         */
        @jakarta.annotation.Nonnull
        public static CreateChatCompletionRequestStop createFromDiscriminatorValue(@jakarta.annotation.Nonnull final ParseNode parseNode) {
            Objects.requireNonNull(parseNode);
            final CreateChatCompletionRequestStop result = new CreateChatCompletionRequestStop();
            final ParseNode mappingValueNode = parseNode.getChildNode("");
            if (mappingValueNode != null) {
                final String mappingValue = mappingValueNode.getStringValue();
            }
            if (parseNode.getStringValue() != null) {
                result.setString(parseNode.getStringValue());
            }
            return result;
        }
        /**
         * The deserialization information for the current model
         * @return a Map<String, java.util.function.Consumer<ParseNode>>
         */
        @jakarta.annotation.Nonnull
        public Map<String, java.util.function.Consumer<ParseNode>> getFieldDeserializers() {
            return new HashMap<String, java.util.function.Consumer<ParseNode>>();
        }
        /**
         * Gets the string property value. Composed type representation for type string
         * @return a String
         */
        @jakarta.annotation.Nullable
        public String getString() {
            return this.string;
        }
        /**
         * Serializes information the current object
         * @param writer Serialization writer to use to serialize this model
         */
        public void serialize(@jakarta.annotation.Nonnull final SerializationWriter writer) {
            Objects.requireNonNull(writer);
            if (this.getString() != null) {
                writer.writeStringValue(null, this.getString());
            }
        }
        /**
         * Sets the string property value. Composed type representation for type string
         * @param value Value to set for the string property.
         */
        public void setString(@jakarta.annotation.Nullable final String value) {
            this.string = value;
        }
    }
}
